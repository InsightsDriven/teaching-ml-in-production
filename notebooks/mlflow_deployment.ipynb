{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING IN PRODUCTION MADRID - MLFLOW DEPLOYMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons we've seen how to put a simple Scikit-Learn model into production. However, in the real world the models used to be complicated, maybe not Sklearn flavor and there is an important feature engineering of the input data.\n",
    "\n",
    "You can also handle that with MLFlow. We'll see how to do it in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model to Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is defining the paths to the pickle data we saved in previous lessons, in order to be able to reproduce the prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data_path = '../output/pickle_data'\n",
    "\n",
    "artifacts = {\n",
    "    'encoder_path': f'{pickle_data_path}/encoder.pickle',\n",
    "    'umap_path': f'{pickle_data_path}/umap.pickle',\n",
    "    'hdbscan_path': f'{pickle_data_path}/hdbscan.pickle',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put a model into production with MLFlow it is necessary to define a wrapper for it. The process is straightforward with a Scikit-Learn model (KMeans from previous lessons) since the Sklearn Wrapper has been already defined by MLFlow developers.\n",
    "\n",
    "Thus, the only thing we need to do is extend the mlflow.pyfunc.PythonModel class and override the predict method:\n",
    "\n",
    "```python\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        your_code_here\n",
    "    \n",
    "```\n",
    "\n",
    "In the cell below, a custom mlflow.pyfunc.PythonModel has been defined. However, it is more complex than the previous definition since the feature engineering of the input data is also included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/ubuntu/miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import hdbscan\n",
    "\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    # define some useful list of columns\n",
    "    def __init__(self):\n",
    "\n",
    "        self.columns_to_encode = ['origin', 'destination', 'train_type', 'train_class', 'fare']\n",
    "        self.columns_to_remove = ['insert_date', 'start_date', 'end_date']\n",
    "\n",
    "    # at the time of loading the MLFlow model, the pickle data from the baseline\n",
    "    # pipeline has to be loaded\n",
    "    def load_context(self, context):\n",
    "        \n",
    "        with open(context.artifacts['encoder_path'], 'rb') as f:\n",
    "            self.encoder_m = pickle.load(f)\n",
    "            \n",
    "        with open(context.artifacts['umap_path'], 'rb') as f:\n",
    "            self.umap_m = pickle.load(f)\n",
    "        \n",
    "        with open(context.artifacts['hdbscan_path'], 'rb') as f:\n",
    "            self.hdbscan_m = pickle.load(f)\n",
    "            \n",
    "    # the datetime columns could arrive in the integer form, in that case convert to\n",
    "    # datetime type\n",
    "    def check_dt_type(self, model_input):\n",
    "        \n",
    "        if model_input[self.columns_to_remove[0]].dtype == 'int64':\n",
    "            for col in self.columns_to_remove:\n",
    "                model_input[col] = pd.to_datetime(model_input[col])\n",
    "        \n",
    "        return model_input\n",
    "\n",
    "    # the baseline transformations are done here\n",
    "    def transform(self, model_input):\n",
    "        \n",
    "        model_input.dropna(inplace=True)\n",
    "        \n",
    "        model_input = self.check_dt_type(model_input)\n",
    "        \n",
    "        model_input.loc[:, self.columns_to_encode] = \\\n",
    "            self.encoder_m.transform(model_input[self.columns_to_encode])\n",
    "        \n",
    "        model_input['duration'] = (model_input['end_date'] - model_input['start_date']).dt.seconds / 3600\n",
    "\n",
    "        model_input['time_to_departure'] = (model_input['start_date'].dt.tz_localize('Europe/Madrid').dt.tz_convert('UTC') \\\n",
    "                                   - model_input['insert_date'].dt.tz_localize('UTC')).dt.days\n",
    "\n",
    "        model_input['hour'] = model_input['start_date'].dt.hour\n",
    "\n",
    "        model_input['weekday'] = model_input['start_date'].dt.dayofweek\n",
    "\n",
    "        model_input = model_input[[x for x in model_input.columns if x not in self.columns_to_remove]]\n",
    "        \n",
    "        return model_input\n",
    "\n",
    "    # main method to override, the OrdinalEncoder and UMAP transformations are done along\n",
    "    # with the HDBSCAN prediction over this embedding\n",
    "    def predict(self, context, model_input):\n",
    "        \n",
    "        # allocate payload with return value for null\n",
    "        payload = np.ones(len(model_input)) * -1\n",
    "        \n",
    "        preprocessed = self.transform(model_input.reset_index(drop=True))\n",
    "        embedding = self.umap_m.transform(preprocessed)\n",
    "        clusters, _ = hdbscan.approximate_predict(self.hdbscan_m, embedding)\n",
    "        \n",
    "        # fill not null records with their cluster\n",
    "        payload[preprocessed.index] = clusters\n",
    "        \n",
    "        return payload\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the custom model has been defined, it is necessary to pack everything together, both the model and the conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_pyfunc_model_path = '../output/custom_model'\n",
    "\n",
    "# remove all models if already there\n",
    "!rm -rf $mlflow_pyfunc_model_path\n",
    "\n",
    "# conda environment definition\n",
    "conda_env = {\n",
    "    'channels': ['defaults'],\n",
    "    'dependencies': [\n",
    "        'python',\n",
    "        {'pip': [\n",
    "            'mlflow',\n",
    "            'umap-learn',\n",
    "            'hdbscan',\n",
    "          ]\n",
    "        },\n",
    "    ],\n",
    "    'name': 'custom_env',\n",
    "}\n",
    "\n",
    "# finally save the model as an MLFlow project into the output directory\n",
    "mlflow.pyfunc.save_model(path=mlflow_pyfunc_model_path, \n",
    "                         python_model=ModelWrapper(),\n",
    "                         conda_env=conda_env,\n",
    "                         artifacts=artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons we saw how to create an endpoint with MLFlow and the command line:\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m path_to_your_model -h host -p port\n",
    "```\n",
    "\n",
    "However, it is desirable that this endpoint could be always alive. This can be done with systemd and the following configuration:\n",
    "\n",
    "```\n",
    "[Unit]\n",
    "Description=MLFlow model in production\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "Restart=on-failure\n",
    "RestartSec=30\n",
    "StandardOutput=file:/var/log/mlflow/production/stdout.log\n",
    "StandardError=file:/var/log/mlflow/production/stderr.log\n",
    "Environment=MLFLOW_TRACKING_URI=http://host_ts:port_ts\n",
    "Environment=MLFLOW_CONDA_HOME=/home/ubuntu/miniconda3\n",
    "ExecStart=/bin/bash -c 'PATH=/home/ubuntu/miniconda3/envs/mlinproduction_env/bin/:$PATH exec mlflow models serve -m path_to_your_model -h host -p port'\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing the endpoint it is necessary to load some test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insert_date</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>train_type</th>\n",
       "      <th>price</th>\n",
       "      <th>train_class</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4266349</td>\n",
       "      <td>2019-04-19 11:02:07</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-05-15 07:40:00</td>\n",
       "      <td>2019-05-15 10:05:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>47.30</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6261496</td>\n",
       "      <td>2019-05-15 01:20:47</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-05-29 11:00:00</td>\n",
       "      <td>2019-05-29 13:45:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90.50</td>\n",
       "      <td>Turista Plus</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9583320</td>\n",
       "      <td>2019-06-20 17:25:44</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>VALENCIA</td>\n",
       "      <td>2019-07-28 16:55:00</td>\n",
       "      <td>2019-07-28 21:58:00</td>\n",
       "      <td>AVE-LD</td>\n",
       "      <td>39.25</td>\n",
       "      <td>Turista con enlace</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7197242</td>\n",
       "      <td>2019-05-25 07:48:22</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>2019-06-12 10:00:00</td>\n",
       "      <td>2019-06-12 12:32:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>53.40</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10340229</td>\n",
       "      <td>2019-08-15 03:08:13</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>VALENCIA</td>\n",
       "      <td>2019-08-28 16:55:00</td>\n",
       "      <td>2019-08-28 19:14:00</td>\n",
       "      <td>ALVIA</td>\n",
       "      <td>46.15</td>\n",
       "      <td>Preferente</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7436516</td>\n",
       "      <td>2019-05-27 17:40:21</td>\n",
       "      <td>VALENCIA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-07-08 12:40:00</td>\n",
       "      <td>2019-07-08 14:20:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>45.30</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3724117</td>\n",
       "      <td>2019-04-15 14:08:30</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-04-25 12:00:00</td>\n",
       "      <td>2019-04-25 15:10:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>58.15</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7481454</td>\n",
       "      <td>2019-05-28 05:43:10</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>2019-07-15 08:30:00</td>\n",
       "      <td>2019-07-15 11:14:00</td>\n",
       "      <td>ALVIA</td>\n",
       "      <td>87.40</td>\n",
       "      <td>Preferente</td>\n",
       "      <td>Flexible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488848</td>\n",
       "      <td>2019-08-26 11:29:18</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>VALENCIA</td>\n",
       "      <td>2019-10-21 15:10:00</td>\n",
       "      <td>2019-10-21 16:52:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>21.95</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8780694</td>\n",
       "      <td>2019-06-11 14:03:46</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>2019-06-27 13:25:00</td>\n",
       "      <td>2019-06-27 16:24:00</td>\n",
       "      <td>AVE-TGV</td>\n",
       "      <td>107.70</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Flexible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 insert_date     origin destination          start_date  \\\n",
       "4266349  2019-04-19 11:02:07    SEVILLA      MADRID 2019-05-15 07:40:00   \n",
       "6261496  2019-05-15 01:20:47  BARCELONA      MADRID 2019-05-29 11:00:00   \n",
       "9583320  2019-06-20 17:25:44     MADRID    VALENCIA 2019-07-28 16:55:00   \n",
       "7197242  2019-05-25 07:48:22     MADRID     SEVILLA 2019-06-12 10:00:00   \n",
       "10340229 2019-08-15 03:08:13     MADRID    VALENCIA 2019-08-28 16:55:00   \n",
       "7436516  2019-05-27 17:40:21   VALENCIA      MADRID 2019-07-08 12:40:00   \n",
       "3724117  2019-04-15 14:08:30  BARCELONA      MADRID 2019-04-25 12:00:00   \n",
       "7481454  2019-05-28 05:43:10     MADRID     SEVILLA 2019-07-15 08:30:00   \n",
       "488848   2019-08-26 11:29:18     MADRID    VALENCIA 2019-10-21 15:10:00   \n",
       "8780694  2019-06-11 14:03:46     MADRID   BARCELONA 2019-06-27 13:25:00   \n",
       "\n",
       "                    end_date train_type   price         train_class      fare  \n",
       "4266349  2019-05-15 10:05:00        AVE   47.30             Turista     Promo  \n",
       "6261496  2019-05-29 13:45:00        AVE   90.50        Turista Plus     Promo  \n",
       "9583320  2019-07-28 21:58:00     AVE-LD   39.25  Turista con enlace     Promo  \n",
       "7197242  2019-06-12 12:32:00        AVE   53.40             Turista     Promo  \n",
       "10340229 2019-08-28 19:14:00      ALVIA   46.15          Preferente     Promo  \n",
       "7436516  2019-07-08 14:20:00        AVE   45.30             Turista     Promo  \n",
       "3724117  2019-04-25 15:10:00        AVE   58.15             Turista     Promo  \n",
       "7481454  2019-07-15 11:14:00      ALVIA   87.40          Preferente  Flexible  \n",
       "488848   2019-10-21 16:52:00        AVE   21.95             Turista     Promo  \n",
       "8780694  2019-06-27 16:24:00    AVE-TGV  107.70             Turista  Flexible  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../data/raw/renfe.parquet')\n",
    "\n",
    "test_data = df.sample(10)\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the endpoint is not working as expected, the model can be loaded with the MLFlow API into the Jupyter notebook and start debugging it with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py:281: DeprecationWarning: .. Warning:: ``mlflow.pyfunc.load_pyfunc`` is deprecated since 1.0. This method will be removed in a near future release. Use ``mlflow.pyfunc.load_model`` instead.\n",
      "  return load_pyfunc(model_uri, suppress_warnings)\n",
      "/home/ubuntu/miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/umap/nndescent.py\", line 124:\n",
      "    @numba.njit(parallel=True)\n",
      "    def init_from_random(n_neighbors, data, query_points, heap, rng_state):\n",
      "    ^\n",
      "\n",
      "  self.func_ir.loc))\n",
      "/home/ubuntu/miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../miniconda3/envs/mlinproduction_env/lib/python3.7/site-packages/umap/nndescent.py\", line 135:\n",
      "    @numba.njit(parallel=True)\n",
      "    def init_from_tree(tree, data, query_points, heap, rng_state):\n",
      "    ^\n",
      "\n",
      "  self.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [8. 4. 8. 8. 8. 8. 7. 4. 9. 5.]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n",
    "\n",
    "print(f'Predictions: {loaded_model.predict(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is done via Python requests, however it can also be done with cURL or another tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [8.0, 4.0, 8.0, 8.0, 8.0, 8.0, 7.0, 4.0, 9.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host = 'localhost'\n",
    "port = 8001\n",
    "\n",
    "url = f'http://{host}:{port}/invocations'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "r = requests.post(url=url, headers=headers, data=test_data.to_json(orient='split'))\n",
    "\n",
    "print(f'Predictions: {r.text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
